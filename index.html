
<!DOCTYPE html>
<html lang="en">
<head>
    <title>Chengguang XU</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="blog, accent, , Chengguang XU, jekyll">
    <meta name="author" content="">


    <meta name="description" content="Human Learning">
    <link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'>
    <link rel="alternate" type="application/rss+xml" title="Chengguang XU RSS" href="/feed.xml" />
    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/pub.css">


    <!-- Facebook Open Graph -->
    <meta name="og:description" content="Human Learning">
    <meta name="og:title" content="Chengguang XU">
    <meta name="og:url" content="cgxuvector.github.io/">
    <meta name="og:type" content="article">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Chengguang XU">
    <meta name="twitter:description" content="Human Learning">
    <meta name="twitter:url" content="cgxuvector.github.io/">

    <meta name="twitter:image" content="">

</head>
<body>
<div class="wrapper">
    <table style="width:800px;">
        <tbody>
        <tr>
            <td ><img src="profile/profile_img.jpeg" style="width:120px; height:170px"></td>
            <td valign="TOP" align="RIGHT">
                <h2>Chengguang Xu</h2>
                <b>Ph.D. Candidate</b><br>
                &nbsp; <b>Khoury College of Computer Sciences</b><b><br>
                Northeastern University</b><br>
                <p class="logo-text">
                </p>
                <a href="https://www.linkedin.com/in/chengguang-xu-2322171b7/"><img src="figures/linkedin.png" alt="Linkedin"
                                                                                    height="25" width="25"></a>
                <a href="https://scholar.google.com/citations?user=F_yEpGIAAAAJ&hl=en"><img src="figures/google_scholar.jpeg" alt="Google Scholar"
                                                                                            height="24" width="24"></a>
                <a href="https://github.com/cgxuvector?tab=repositories"><img src="figures/github.jpeg" alt="Github"
                                                                                            height="24" width="24"></a>
            </td>
        </tr>
        </tbody>
    </table>

    <div class="container content">

        <h2 id="about-me">About Me</h2>

        <p>你好 (Hi),<br />
            I am a Ph.D. candidate at Khoury College of Computer Sciences, Northeastern University, advised by
            <a href="https://www.khoury.northeastern.edu/people/chris-amato/", style="color:darkolivegreen">Prof. Christopher Amato</a>
            and
            <a href="ttps://www.khoury.northeastern.edu/people/lawson-wong/", style="color:darkolivegreen">Prof. Lawson Wong</a>.
        </p>

        <p>Previously, I received my B.S. and M.S. from Nankai University (China) where I was fortunate to work with
            <a href="https://ai.nankai.edu.cn/info/1032/2779.htm", style="color:darkolivegreen">Prof. Feng Duan</a>
            on computer vision and autonomous driving.
        </p>

        <h2 id="contact">Contact</h2>
        <p>If you are interested in my research or collaboration, I can be reached via:</p>
        <ul>
            <li>Email: xu [dot] cheng [at] northeastern [dot] edu</li>
        </ul>

        <h2 id="research-interest">Research Interest</h2>
        <p>
            <strong><em><span style="color:darkolivegreen">Currently, I am working on visual navigation with rough priors (e.g., 2-D rough maps) to facilitate fast generalization to unseen environments, using deep learning and computer vision.
            </span></em></strong>.<br />
        </p>

        <h2 id="publications">Publications</h2>

        <table>
            <tr>
                <td>
                    <br />
                    <img src="figures/hierarchical_nav_corl20.png" class="center", height="120", width="320" />
                </td>
                <td>
                    <center>
                        <a href="https://arxiv.org/pdf/2106.03665.pdf" style="color:black"><b>Hierarchical Robot Navigation
                            in Novel Environments using Rough 2-D Maps</b></a><br />
                    </center>

                    <p>
                        <center>
                    <b>Chengguang Xu</b>, Christopher Amato and Lawson L.S. Wong. In the <i> Proceedings of the
                    Conference on Robot Learning</i> (<b>CoRL-20</b>), November 2020.
                            <a href="https://corlconf.github.io/corl2020/paper_442/", style="color:darkolivegreen">[paper, code, and video]</a>
                        </center>
                    </p>
                    <p>
                        <br /><u>Description:</u> Given the rough 2-D map of an unseen maze, a dynamic topological
                        map is first initialized from it. During navigation, the dynamic topological map proposes a local map
                        patch m of the next landmark to reach. Next, the generative model uses m to generate a first-person
                        observation og corresponding to the landmark. The local controller then uses og as a subgoal and
                        executes the local navigation. If the move is successful, the local map patch of next landmark is
                        generated; otherwise, the dynamic map is updated, and another path is planned and executed.
                    </p>
                </td>
            </tr>


            <tr>
                <td>
                    <br />
                    <img src="figures/SupFL.png" class="center", height="180", width="320" />
                </td>
                <td>
                    <center>
                        <a href="https://arxiv.org/pdf/2106.03665.pdf" style="color:black"><b>Deep Supervised Summarization:
                            Algorithm and Application to Learning Instructions</b></a><br />
                    </center>

                    <p>
                    <center>
                    <b>Chengguang Xu</b> and Ehsan Elhamifar. In the Proceedings of the <i> Thirty-Third Conference on Neural
                    Information Processing Systems </i> (<b>NeurIPS-19</b>),
                        December 2019. <a href="https://openreview.net/pdf?id=B1gaN4Hg8H", style="color:darkolivegreen">[Paper]</a>
                    </center>
                    </p>
                    <p>
                        <br /><u>Description:</u> We addressed the problem of supervised subset selection by generalizing the facility location to learn
                        from ground-truth summaries. We considered an efficient sparse optimization of the uncapacitated
                        facility location and investigated conditions under which it recovers ground-truth representatives
                        and also becomes equivalent to the original NP-hard problem. We designed a loss function and an
                        efficient framework to learn representations of data so that the input of transformed data to the facility
                        location satisfies the theoretical conditions, hence, recovers ground-truth summaries. We showed
                        the effectiveness of our method for recovering key-steps of instructional videos.
                    </p>
                </td>
            </tr>

            <tr>
                <td>
                    <br />
                    <img src="figures/bci_vision_fusion.png" class="center", height="120", width="320" />
                </td>
                <td>
                    <center>
                        <a href="https://arxiv.org/pdf/2106.03665.pdf" style="color:black"><b>A human-vehicle
                            collaborative simulated driving system based on hybrid brain–computer interfaces and computer vision</b></a><br />
                    </center>

                    <p>
                    <center>
                    Wenyu Li, Feng Duan, Shili Sheng, <b>Chengguang Xu</b>, Rensong Liu, Zhiwen Zhang, Xue Jiang. In the <i> IEEE Transactions on Cognitive and Developmental Systems</i>, October 2017.
                    <a href="https://ieeexplore.ieee.org/abstract/document/8082510", style="color:darkolivegreen">[paper]</a>
                    </center>
                    </p>
                    <p>
                        <br /><u>Description:</u> Automatic driving vehicles have been developed to provide more convenient and comfortable driving experiences.
                        However, these vehicles failed in satisfying the variance of human intentions. Recently, the strategy of collaborating brain-computer
                        interface (BCI) controlling and automatic driving receives attention. Since the BCI system remained some limitation in real-time controlling,
                        a fusion method has been proposed to explore and verify the feasibility of human-vehicle collaborative driving in this paper. A hybrid BCI was developed to interpret human intentions.
                        In addition, a computer vision-based automatic driving component was developed to maintain the vehicle on the road.
                        A system for fusing these two kinds of vehicle driving decisions was first proposed in this paper. This system can simultaneously obtain the visual data and the hybrid electroencephalograph (EEG) signals.
                        The hybrid EEG signals consist of steady-state visual evoked potentials and motor imagery. The obtained multisource information can be fused to make the final decision to drive a simulated vehicle.
                        The proposed system was evaluated with different destinations. The experimental results verify the feasibility of fusing both human intention and computer vision. The task success rate reached 91.1% and the information transfer rate was 85.80 bit/min
                    </p>
                </td>
            </tr>

            <tr>
                <td>
                    <br />
                    <center>
                    <img src="figures/turtlebot_ccdc.png" class="center", height="260", width="200" />
                    </center>
                </td>
                <td>
                    <center>
                        <a href="https://www.researchgate.net/profile/Chengguang-Xu/publication/318695036_Developing_an_identity_recognition_low-cost_home_service_robot_based_on_turtlebot_and_ROS/links/5a3226da0f7e9b2a282d59c8/Developing-an-identity-recognition-low-cost-home-service-robot-based-on-turtlebot-and-ROS.pdf" style="color:black"><b>Developing an identity
                            recognition low-cost home service robot based on turtlebot and ROS</b></a><br />
                    </center>

                    <p>
                    <center>
                    <b>Chengguang Xu</b>, Wenyu Li, Jeffrey Too Chuan Tan, Zengqiang Chen, Han Zhang, Feng Duan.
                    In the <i>Twenty-29th Chinese Control And Decision Conference</i> (<b>CCDC-17</b>), May 2017.
                    <a href="https://www.researchgate.net/profile/Chengguang-Xu/publication/318695036_Developing_an_identity_recognition_low-cost_home_service_robot_based_on_turtlebot_and_ROS/links/5a3226da0f7e9b2a282d59c8/Developing-an-identity-recognition-low-cost-home-service-robot-based-on-turtlebot-and-ROS.pdf", style="color:darkolivegreen">[paper]</a>

                    </center>
                    </p>
                    <p>
                        <br /><u>Description:</u> Aging population and disabled assistance have become serious social problems these years. However, it is
                        impossible to make sure that every old people has a caregiver. Therefore, the home service robot is a good method to
                        mitigate these issues. Although current robotic technology is mature, service robots with advanced functions are very
                        expensive. In this paper, we developed an identity recognition low-cost home service robot based on Turtlebot and
                        Robot Operating System (ROS). In order to verify the performance of the service robot, we designed a series of
                        experiments in a simulated home environment. The average successful rates of the designed experiments were all above
                        90%. The results confirmed that the low-cost service robot satisfied our design target.
                    </p>
                </td>
            </tr>
        </table>
    </div>
</div>

</body>
<footer>
</footer>
